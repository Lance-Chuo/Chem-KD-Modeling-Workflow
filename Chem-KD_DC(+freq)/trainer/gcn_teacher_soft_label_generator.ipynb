{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc26803-1ca9-467d-bacf-4b85b1198946",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import global_mean_pool, GCNConv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import threading\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "data_lock = threading.Lock()\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "class GCNTeacher(nn.Module):\n",
    "    def __init__(self, node_dim: int, global_dim: int, hidden_dims=[128, 128], dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.norm = nn.BatchNorm1d(node_dim)\n",
    "        self.convs = nn.ModuleList()\n",
    "        in_dim = node_dim\n",
    "        for h in hidden_dims:\n",
    "            self.convs.append(GCNConv(in_dim, h))\n",
    "            in_dim = h\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        if global_dim:\n",
    "            self.global_norm = nn.BatchNorm1d(global_dim)\n",
    "            self.global_mlp = nn.Sequential(\n",
    "                nn.Linear(global_dim, 128), nn.ReLU(), nn.Dropout(dropout)\n",
    "            )\n",
    "            self.final_dim = hidden_dims[-1] + 128\n",
    "        else:\n",
    "            self.final_dim = hidden_dims[-1]\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(self.final_dim, self.final_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(self.final_dim // 2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, data: Data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        x = self.norm(x)\n",
    "        for conv in self.convs:\n",
    "            x = F.relu(conv(x, edge_index))\n",
    "            x = self.dropout(x)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        if hasattr(data, 'u') and data.u is not None:\n",
    "            u = self.global_norm(data.u)\n",
    "            u = self.global_mlp(u)\n",
    "            x = torch.cat([x, u], dim=1)\n",
    "        return self.output(x).squeeze()\n",
    "\n",
    "def load_graphs(path: str):\n",
    "    print(f\"[Single-threaded] Loading graph data: {path}\")\n",
    "    data = torch.load(os.path.join(path, 'graph_data.pt'), map_location='cpu')\n",
    "    print(f\"Number of samples: {len(data)} | Node dimension: {data[0]['x'].shape[1]}\")\n",
    "    return data\n",
    "\n",
    "def make_loader(graphs, batch_size, shuffle=True):\n",
    "    return DataLoader(\n",
    "        [Data(**g) for g in graphs],\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=0,\n",
    "        pin_memory=False,\n",
    "        prefetch_factor=None\n",
    "    )\n",
    "\n",
    "def print_gpu_memory(desc: str):\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / (1024 ** 3)\n",
    "        cached = torch.cuda.memory_reserved() / (1024 ** 3)\n",
    "        print(f\"[GPU Memory] {desc} | Allocated: {allocated:.2f}GB | Cached: {cached:.2f}GB\")\n",
    "\n",
    "def train_and_eval(graphs, split_seed, learn_seed, dropout, batch_size, epochs=200, patience=50):\n",
    "    print(f\"\\n[Training Config] split_seed={split_seed}, learn_seed={learn_seed}, dropout={dropout}, batch_size={batch_size}\")\n",
    "    \n",
    "    device = torch.device('cuda')\n",
    "    print(f\"[Device] Using GPU: {torch.cuda.get_device_name(0)} (ID: {torch.cuda.current_device()})\")\n",
    "\n",
    "    labels = [g['label'].item() for g in graphs]\n",
    "    try:\n",
    "        idx_train, idx_val = train_test_split(\n",
    "            np.arange(len(graphs)), test_size=0.1, random_state=split_seed,\n",
    "            shuffle=True, stratify=labels\n",
    "        )\n",
    "        print(\"[Sampling Strategy] ✅ Stratified Sampling\")\n",
    "    except ValueError as e:\n",
    "        print(f\"[Sampling Strategy] ⚠️ Stratified Sampling Failed: {str(e)}, Switching to Random Sampling\")\n",
    "        idx_train, idx_val = train_test_split(\n",
    "            np.arange(len(graphs)), test_size=0.1, random_state=split_seed, shuffle=True\n",
    "        )\n",
    "    \n",
    "    train_graphs = [graphs[i] for i in idx_train]\n",
    "    val_graphs = [graphs[i] for i in idx_val]\n",
    "\n",
    "    ys = torch.stack([g['y'] for g in train_graphs])\n",
    "    y_mean, y_std = ys.mean().item(), ys.std().item() + 1e-8\n",
    "    for g in train_graphs + val_graphs:\n",
    "        g['y'] = (g['y'] - y_mean) / y_std\n",
    "    del ys\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    train_loader = make_loader(train_graphs, batch_size)\n",
    "    val_loader = make_loader(val_graphs, batch_size, shuffle=False)\n",
    "\n",
    "    set_seed(learn_seed)\n",
    "    sample = train_graphs[0]\n",
    "    node_dim = sample['x'].size(1)\n",
    "    global_dim = sample['u'].size(1) if sample.get('u', None) is not None else 0\n",
    "    model = GCNTeacher(node_dim, global_dim, dropout=dropout).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    best_r2, best_state = -np.inf, None\n",
    "    no_improve_epochs = 0\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        train_losses, train_preds, train_trues = [], [], []\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device, non_blocking=False)\n",
    "            pred = model(batch)\n",
    "            loss = criterion(pred, batch.y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "            train_preds.append(pred.detach().cpu().numpy())\n",
    "            train_trues.append(batch.y.cpu().numpy())\n",
    "            \n",
    "            del batch, pred, loss\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        train_r2 = r2_score(np.concatenate(train_trues), np.concatenate(train_preds))\n",
    "        train_loss = np.mean(train_losses)\n",
    "        del train_losses, train_preds, train_trues\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        model.eval()\n",
    "        val_losses, val_preds, val_trues = [], [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = batch.to(device, non_blocking=False)\n",
    "                pred = model(batch)\n",
    "                val_losses.append(criterion(pred, batch.y).item())\n",
    "                val_preds.append(pred.cpu().numpy())\n",
    "                val_trues.append(batch.y.cpu().numpy())\n",
    "                \n",
    "                del batch, pred\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        val_r2 = r2_score(np.concatenate(val_trues), np.concatenate(val_preds))\n",
    "        val_loss = np.mean(val_losses)\n",
    "        del val_losses, val_preds, val_trues\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if epoch % 10 == 0 or epoch == 1 or epoch == epochs:\n",
    "            print(f\"[Epoch {epoch:4d}/{epochs}] \"\n",
    "                  f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
    "                  f\"Train R²: {train_r2:.4f} | Val R²: {val_r2:.4f}\")\n",
    "\n",
    "        if val_r2 > best_r2:\n",
    "            best_r2 = val_r2\n",
    "            best_state = model.state_dict()\n",
    "            no_improve_epochs = 0\n",
    "        else:\n",
    "            no_improve_epochs += 1\n",
    "            if no_improve_epochs >= patience:\n",
    "                print(f\"[Early Stopping Triggered] Val R² has no improvement for {patience} consecutive epochs, stopping training\")\n",
    "                break\n",
    "\n",
    "    del model, optimizer, criterion, train_loader, val_loader\n",
    "    del train_graphs, val_graphs\n",
    "    torch.cuda.empty_cache()\n",
    "    print_gpu_memory(f\"Parameter combination (ss={split_seed}, ls={learn_seed}) after training completion\")\n",
    "    print(f\"[Training Result] Best Val R²: {best_r2:.4f}\")\n",
    "\n",
    "    return best_r2, best_state, y_mean, y_std, node_dim, global_dim\n",
    "\n",
    "def generate_predictions(model, graphs, y_mean, y_std, batch_size=64):\n",
    "    print(\"\\n[Prediction] Starting to generate results\")\n",
    "    loader = make_loader(graphs, batch_size, shuffle=False)\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "\n",
    "    normalized_preds = []\n",
    "    denormalized_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"[Prediction Progress]\"):\n",
    "            batch = batch.to(device, non_blocking=False)\n",
    "            p_normalized = model(batch)\n",
    "            \n",
    "            p_normalized_cpu = p_normalized.cpu().numpy()\n",
    "            p_denormalized_cpu = p_normalized_cpu * y_std + y_mean\n",
    "            \n",
    "            normalized_preds.extend(p_normalized_cpu)\n",
    "            denormalized_preds.extend(p_denormalized_cpu)\n",
    "            \n",
    "            del batch, p_normalized, p_normalized_cpu\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"[Prediction] Completed\")\n",
    "    return np.array(normalized_preds), np.array(denormalized_preds)\n",
    "\n",
    "def process_feature_directory(feat_dir, predict_dir, output_dir, epochs, split_seeds, learn_seeds, dropouts, batch_sizes, \n",
    "                              all_normalized_labels, all_denormalized_labels):\n",
    "    print(f\"\\n\" + \"=\"*50)\n",
    "    print(f\"[Feature Processing] Starting to process: {feat_dir}\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        graphs = load_graphs(feat_dir)\n",
    "        feat_name = os.path.basename(feat_dir.rstrip('/\\\\'))\n",
    "        feat_output_dir = os.path.join(output_dir, feat_name)\n",
    "        os.makedirs(feat_output_dir, exist_ok=True)\n",
    "\n",
    "        best_cfg = None\n",
    "        best_r2 = -np.inf\n",
    "        best_state, best_mean, best_std = None, None, None\n",
    "        best_node_dim, best_global_dim, best_dropout = None, None, None\n",
    "\n",
    "        for ss in split_seeds:\n",
    "            for ls in learn_seeds:\n",
    "                for do in dropouts:\n",
    "                    for bs in batch_sizes:\n",
    "                        r2, state, y_m, y_s, node_dim, global_dim = train_and_eval(\n",
    "                            graphs, ss, ls, do, bs, epochs\n",
    "                        )\n",
    "\n",
    "                        model_name = f\"model_ss{ss}_ls{ls}_do{do}_bs{bs}.pt\"\n",
    "                        save_path = os.path.join(feat_output_dir, model_name)\n",
    "                        torch.save({\n",
    "                            'model_state_dict': state, 'node_dim': node_dim, 'global_dim': global_dim,\n",
    "                            'hidden_dims': [128, 128], 'dropout': do, 'model_type': 'gcn',\n",
    "                            'y_mean': y_m, 'y_std': y_s\n",
    "                        }, save_path)\n",
    "\n",
    "                        if r2 > best_r2:\n",
    "                            best_r2 = r2\n",
    "                            best_cfg = (ss, ls, do, bs)\n",
    "                            best_state = state\n",
    "                            best_mean = y_m\n",
    "                            best_std = y_s\n",
    "                            best_node_dim = node_dim\n",
    "                            best_global_dim = global_dim\n",
    "                            best_dropout = do\n",
    "\n",
    "                        del state\n",
    "                        torch.cuda.empty_cache()\n",
    "                        print(f\"[Hyperparameter] Completed configuration (ss={ss}, ls={ls}, do={do}, bs={bs}), current best R²: {best_r2:.4f}\")\n",
    "\n",
    "        ss, ls, do, bs = best_cfg\n",
    "        best_name = f\"{feat_name}_best_ss{ss}_ls{ls}_do{do}_bs{bs}.pt\"\n",
    "        best_path = os.path.join(feat_output_dir, best_name)\n",
    "        torch.save({\n",
    "            'model_state_dict': best_state, 'node_dim': best_node_dim, 'global_dim': best_global_dim,\n",
    "            'hidden_dims': [128, 128], 'dropout': best_dropout, 'model_type': 'gcn',\n",
    "            'y_mean': best_mean, 'y_std': best_std\n",
    "        }, best_path)\n",
    "\n",
    "        best_models_dir = os.path.join(output_dir, \"best_models\")\n",
    "        os.makedirs(best_models_dir, exist_ok=True)\n",
    "        shutil.copyfile(best_path, os.path.join(best_models_dir, best_name))\n",
    "        print(f\"[Best Model] Save path: {best_path}\")\n",
    "\n",
    "        graphs_pred = load_graphs(predict_dir)\n",
    "        best_model = GCNTeacher(\n",
    "            node_dim=best_node_dim, global_dim=best_global_dim,\n",
    "            hidden_dims=[128, 128], dropout=best_dropout\n",
    "        ).to(torch.device('cuda'))\n",
    "        best_model.load_state_dict(best_state)\n",
    "\n",
    "        normalized_preds, denormalized_preds = generate_predictions(\n",
    "            best_model, graphs_pred, best_mean, best_std\n",
    "        )\n",
    "\n",
    "        normalized_csv = os.path.join(feat_output_dir, 'predict_normalized_labels.csv')\n",
    "        pd.DataFrame({f\"{feat_name}_teacher_normalized\": normalized_preds}).to_csv(normalized_csv, index=False)\n",
    "        \n",
    "        denormalized_csv = os.path.join(feat_output_dir, 'predict_denormalized_labels.csv')\n",
    "        pd.DataFrame({f\"{feat_name}_teacher_denormalized\": denormalized_preds}).to_csv(denormalized_csv, index=False)\n",
    "\n",
    "        with data_lock:\n",
    "            all_normalized_labels[feat_name] = normalized_preds\n",
    "            all_denormalized_labels[feat_name] = denormalized_preds\n",
    "\n",
    "        del graphs, graphs_pred, best_state, normalized_preds, denormalized_preds\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        cost_time = time.time() - start_time\n",
    "        print(f\"\\n[Feature Processing] ✅ Completed: {feat_name} | Time consumed: {cost_time:.2f} seconds\")\n",
    "        print(f\"[Output Path] Normalized labels: {normalized_csv}\")\n",
    "        print(f\"[Output Path] Denormalized labels: {denormalized_csv}\")\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "        return True, feat_name\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n[Feature Processing] ❌ Failed: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "        torch.cuda.empty_cache()\n",
    "        return False, os.path.basename(feat_dir)\n",
    "\n",
    "def main(base_dir: str, predict_dir: str, output_dir: str, epochs=200):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(\"=\"*60)\n",
    "    print(f\"[Main Program] Started (Pure Single-threaded Mode)\")\n",
    "    print(f\"[Path Configuration] Feature base directory: {base_dir}\")\n",
    "    print(f\"[Path Configuration] Prediction data directory: {predict_dir}\")\n",
    "    print(f\"[Path Configuration] Output directory: {output_dir}\")\n",
    "    print(f\"[Training Configuration] Total epochs: {epochs} | Device: GPU (Forced)\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "    feat_dirs = []\n",
    "    for d in os.listdir(base_dir):\n",
    "        dir_path = os.path.join(base_dir, d)\n",
    "        if os.path.isdir(dir_path):\n",
    "            feat_dirs.append(dir_path)\n",
    "    num_features = len(feat_dirs)\n",
    "    print(f\"[Data Statistics] Detected {num_features} feature directories, will process them sequentially in single thread\\n\")\n",
    "\n",
    "    all_normalized_labels = defaultdict(list)\n",
    "    all_denormalized_labels = defaultdict(list)\n",
    "\n",
    "    split_seeds = [0, 1, 2]\n",
    "    learn_seeds = [0, 1, 2]\n",
    "    dropouts = [0.1, 0.2, 0.3]\n",
    "    batch_sizes = [32, 64]\n",
    "\n",
    "    successful_features = []\n",
    "    for idx, feat_dir in enumerate(feat_dirs, 1):\n",
    "        print(f\"[Main Program] Starting to process the {idx}/{num_features} feature directory\")\n",
    "        success, name = process_feature_directory(\n",
    "            feat_dir, predict_dir, output_dir, epochs,\n",
    "            split_seeds, learn_seeds, dropouts, batch_sizes,\n",
    "            all_normalized_labels, all_denormalized_labels\n",
    "        )\n",
    "        if success:\n",
    "            successful_features.append(name)\n",
    "\n",
    "    normalized_df = pd.DataFrame(all_normalized_labels)\n",
    "    all_normalized_path = os.path.join(output_dir, \"all_normalized_labels.csv\")\n",
    "    normalized_df.to_csv(all_normalized_path, index=False)\n",
    "\n",
    "    denormalized_df = pd.DataFrame(all_denormalized_labels)\n",
    "    all_denormalized_path = os.path.join(output_dir, \"all_denormalized_labels.csv\")\n",
    "    denormalized_df.to_csv(all_denormalized_path, index=False)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"[Main Program] All processing completed!\")\n",
    "    print(f\"[Result Statistics] Successfully processed: {len(successful_features)}/{num_features} feature directories\")\n",
    "    print(f\"[Summary Output] All normalized labels: {all_normalized_path}\")\n",
    "    print(f\"[Summary Output] All denormalized labels: {all_denormalized_path}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    return all_normalized_path, all_denormalized_path\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if not torch.cuda.is_available():\n",
    "        raise RuntimeError(\"❌ No available GPU detected! Please install NVIDIA driver and matching PyTorch (e.g., cu117 version).\")\n",
    "    \n",
    "    # Input/Output Path Configuration (modify according to actual needs)\n",
    "    base_dir = \"PATH/TO/FEATURE_BASE_DIRECTORY\"\n",
    "    predict_dir = \"PATH/TO/PREDICTION_DATA_DIRECTORY\"\n",
    "    output_dir = \"PATH/TO/OUTPUT_DIRECTORY\"\n",
    "    epochs = 500\n",
    "\n",
    "    try:\n",
    "        normalized_path, denormalized_path = main(base_dir, predict_dir, output_dir, epochs)\n",
    "\n",
    "        print(\"\\n[Result Analysis] Loading summary data...\")\n",
    "        if os.path.exists(normalized_path) and os.path.getsize(normalized_path) > 0:\n",
    "            normalized_df = pd.read_csv(normalized_path)\n",
    "            print(\"\\n[Result Analysis] Normalized labels statistics:\")\n",
    "            print(normalized_df.describe().round(4))\n",
    "        else:\n",
    "            print(\"\\n[Result Analysis] Normalized labels file does not exist or is empty\")\n",
    "\n",
    "        if os.path.exists(denormalized_path) and os.path.getsize(denormalized_path) > 0:\n",
    "            denormalized_df = pd.read_csv(denormalized_path)\n",
    "            print(\"\\n[Result Analysis] Denormalized labels statistics:\")\n",
    "            print(denormalized_df.describe().round(4))\n",
    "        else:\n",
    "            print(\"\\n[Result Analysis] Denormalized labels file does not exist or is empty\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\n[Runtime Error] Program terminated abnormally: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn_gpu (GPU)",
   "language": "python",
   "name": "gnn_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
