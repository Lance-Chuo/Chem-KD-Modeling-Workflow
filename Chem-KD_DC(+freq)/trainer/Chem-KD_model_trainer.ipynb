{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f289504a-04e6-45b5-89e1-d81484535ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import global_mean_pool, GCNConv\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import hashlib\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "\n",
    "# --------------------\n",
    "# 1. Strict Randomness Control\n",
    "# --------------------\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "\n",
    "\n",
    "# --------------------\n",
    "# 2. Model Parameter Hash Calculation\n",
    "# --------------------\n",
    "def get_model_hash(model):\n",
    "    \"\"\"Calculate MD5 hash of model parameters (device-agnostic, exclude dynamic counters)\"\"\"\n",
    "    params = []\n",
    "    for k, v in model.state_dict().items():\n",
    "        if \"num_batches_tracked\" in k:\n",
    "            continue\n",
    "        params.append(v.detach().cpu().float().view(-1))\n",
    "    params = torch.cat(params).numpy()\n",
    "    return hashlib.md5(params.tobytes()).hexdigest()\n",
    "\n",
    "\n",
    "# --------------------\n",
    "# 3. Model Definitions (GCN instead of GNN)\n",
    "# --------------------\n",
    "class SimpleGCN(nn.Module):\n",
    "    def __init__(self, node_dim, edge_dim, global_dim, hidden_dims, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.norm = nn.BatchNorm1d(node_dim)\n",
    "        if edge_dim:\n",
    "            self.edge_norm = nn.BatchNorm1d(edge_dim)\n",
    "        if global_dim:\n",
    "            self.global_norm = nn.BatchNorm1d(global_dim)\n",
    "            self.global_mlp = nn.Sequential(\n",
    "                nn.Linear(global_dim, hidden_dims[-1]),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            )\n",
    "        self.convs = nn.ModuleList()\n",
    "        in_dim = node_dim\n",
    "        for h in hidden_dims:\n",
    "            self.convs.append(GCNConv(in_dim, h))\n",
    "            in_dim = h\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.final_dim = hidden_dims[-1] * (2 if global_dim else 1)\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(self.final_dim, self.final_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(self.final_dim // 2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, data, return_feat=False):\n",
    "        x = self.norm(data.x)\n",
    "        if hasattr(self, 'edge_norm') and hasattr(data, 'edge_attr') and data.edge_attr is not None:\n",
    "            _ = self.edge_norm(data.edge_attr)\n",
    "        u = data.u if hasattr(data, 'u') else None\n",
    "        if u is not None and hasattr(self, 'global_norm'):\n",
    "            u = self.global_norm(u)\n",
    "        \n",
    "        for conv in self.convs:\n",
    "            x = F.relu(conv(x, data.edge_index))\n",
    "            x = self.dropout(x)\n",
    "        \n",
    "        node_pool = global_mean_pool(x, data.batch)\n",
    "        h = torch.cat([node_pool, self.global_mlp(u)], dim=1) if (u is not None and hasattr(self, 'global_mlp')) else node_pool\n",
    "        out = self.output(h).squeeze()\n",
    "        return (out, h) if return_feat else out\n",
    "\n",
    "class EnhancedGCN(nn.Module):\n",
    "    def __init__(self, node_dim, edge_dim, global_dim, hidden_dims, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.node_norm = nn.BatchNorm1d(node_dim)\n",
    "        self.edge_norm = nn.BatchNorm1d(edge_dim) if edge_dim else None\n",
    "        self.global_norm = nn.BatchNorm1d(global_dim) if global_dim else None\n",
    "        if global_dim:\n",
    "            self.global_mlp = nn.Sequential(\n",
    "                nn.Linear(global_dim, hidden_dims[-1]),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            )\n",
    "        self.convs = nn.ModuleList()\n",
    "        in_dim = node_dim\n",
    "        for h in hidden_dims:\n",
    "            self.convs.append(GCNConv(in_dim, h))\n",
    "            in_dim = h\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.final_dim = hidden_dims[-1] * (2 if global_dim else 1)\n",
    "        self.output_mlp = nn.Sequential(\n",
    "            nn.Linear(self.final_dim, self.final_dim//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(self.final_dim//2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, data, return_feat=False):\n",
    "        x = self.node_norm(data.x)\n",
    "        if self.edge_norm and hasattr(data, 'edge_attr') and data.edge_attr is not None:\n",
    "            _ = self.edge_norm(data.edge_attr)\n",
    "        u = getattr(data, 'u', None)\n",
    "        gf = None\n",
    "        if u is not None and self.global_norm is not None:\n",
    "            u = self.global_norm(u)\n",
    "            gf = self.global_mlp(u)\n",
    "        for conv in self.convs:\n",
    "            x = F.relu(conv(x, data.edge_index))\n",
    "            x = self.dropout(x)\n",
    "        pooled = global_mean_pool(x, data.batch)\n",
    "        h = torch.cat([pooled, gf], dim=1) if gf is not None else pooled\n",
    "        out = self.output_mlp(h).squeeze()\n",
    "        return (out, h) if return_feat else out\n",
    "\n",
    "\n",
    "class GateNet(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, num_teachers):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_teachers)\n",
    "    def forward(self, h):\n",
    "        a = F.relu(self.fc1(h))\n",
    "        return F.softmax(self.fc2(a), dim=-1)\n",
    "\n",
    "\n",
    "class Adapter(nn.Module):\n",
    "    def __init__(self, dim_s, dim_t):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(dim_s, dim_t)\n",
    "    def forward(self, h):\n",
    "        return self.linear(h)\n",
    "\n",
    "\n",
    "# --------------------\n",
    "# 4. Data Loader (Reproducible Shuffle)\n",
    "# --------------------\n",
    "def create_data_loader(graph_data, batch_size=32, shuffle=True, seed=42):\n",
    "    \"\"\"Create reproducible data loader with fixed shuffle generator\"\"\"\n",
    "    data_list = []\n",
    "    for graph in graph_data:\n",
    "        x = graph['x'].float() if isinstance(graph['x'], torch.Tensor) else torch.tensor(graph['x'], dtype=torch.float32)\n",
    "        edge_index = graph['edge_index'].long() if isinstance(graph['edge_index'], torch.Tensor) else torch.tensor(graph['edge_index'], dtype=torch.long)\n",
    "        edge_attr = None\n",
    "        if 'edge_attr' in graph and graph['edge_attr'] is not None:\n",
    "            edge_attr = graph['edge_attr'].float() if isinstance(graph['edge_attr'], torch.Tensor) else torch.tensor(graph['edge_attr'], dtype=torch.float32)\n",
    "        u = None\n",
    "        if 'u' in graph and graph['u'] is not None:\n",
    "            u = graph['u'].float() if isinstance(graph['u'], torch.Tensor) else torch.tensor(graph['u'], dtype=torch.float32)\n",
    "        y = graph['y'].float() if isinstance(graph['y'], torch.Tensor) else torch.tensor(graph['y'], dtype=torch.float32)\n",
    "        y_soft = graph.get('y_soft', y).float() if isinstance(graph.get('y_soft', y), torch.Tensor) else torch.tensor(graph.get('y_soft', y), dtype=torch.float32)\n",
    "        \n",
    "        data = Data(\n",
    "            x=x, edge_index=edge_index,\n",
    "            edge_attr=edge_attr, u=u,\n",
    "            y=y, y_soft=y_soft,\n",
    "            idx=graph.get('idx', len(data_list))\n",
    "        )\n",
    "        data_list.append(data)\n",
    "    \n",
    "    if shuffle:\n",
    "        generator = torch.Generator().manual_seed(seed)\n",
    "        return DataLoader(\n",
    "            data_list,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            generator=generator,\n",
    "            num_workers=0\n",
    "        )\n",
    "    else:\n",
    "        return DataLoader(data_list, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "\n",
    "# --------------------\n",
    "# 5. Evaluation and Prediction\n",
    "# --------------------\n",
    "def eval_loader(loader, model, device):\n",
    "    model.eval()\n",
    "    ys, ps = [], []\n",
    "    with torch.no_grad():\n",
    "        for b in loader:\n",
    "            b = b.to(device)\n",
    "            out, _ = model(b, return_feat=True)\n",
    "            ys.append(b.y.view(-1).cpu().numpy())\n",
    "            ps.append(out.cpu().numpy())\n",
    "    ys = np.concatenate(ys)\n",
    "    ps = np.concatenate(ps)\n",
    "    return r2_score(ys, ps), ys, ps\n",
    "\n",
    "\n",
    "def predict_and_evaluate(model, data_loader, y_mean, y_std, device, dataset_name, save_dir, seed=42):\n",
    "    set_seed(seed)\n",
    "    r2_norm, true_norm, pred_norm = eval_loader(data_loader, model, device)\n",
    "    \n",
    "    true_raw = true_norm * y_std + y_mean\n",
    "    pred_raw = pred_norm * y_std + y_mean\n",
    "    r2_raw = r2_score(true_raw, pred_raw)\n",
    "    \n",
    "    print(f\"[{dataset_name}] Normalized RÂ²: {r2_norm:.6f} | Raw Scale RÂ²: {r2_raw:.6f}\")\n",
    "    \n",
    "    # Scatter plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(true_raw, pred_raw, alpha=0.6, label=f'RÂ² = {r2_raw:.4f}')\n",
    "    slope, intercept, _, _, _ = stats.linregress(true_raw, pred_raw)\n",
    "    plt.plot(true_raw, intercept + slope * true_raw, 'r--')\n",
    "    plt.xlabel('True Values', fontname=\"Times New Roman\", fontsize=12)\n",
    "    plt.ylabel('Predicted Values', fontname=\"Times New Roman\", fontsize=12)\n",
    "    plt.title(f'{dataset_name} Prediction Results', fontname=\"Times New Roman\", fontsize=14)\n",
    "    plt.legend(prop={\"family\": \"Times New Roman\"})\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    scatter_path = os.path.join(save_dir, f'{dataset_name}_scatter.png')\n",
    "    plt.savefig(scatter_path, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"Scatter plot saved to: {scatter_path}\")\n",
    "    \n",
    "    # Save prediction results\n",
    "    indices = np.array([data.idx for data in data_loader.dataset])\n",
    "    df = pd.DataFrame({\n",
    "        'Sample_Index': indices,\n",
    "        'True_Value_Raw': true_raw,\n",
    "        'Pred_Value_Raw': pred_raw,\n",
    "        'True_Value_Normalized': true_norm,\n",
    "        'Pred_Value_Normalized': pred_norm\n",
    "    })\n",
    "    csv_path = os.path.join(save_dir, f'{dataset_name}_predictions.csv')\n",
    "    df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"Prediction results saved to: {csv_path}\\n\")\n",
    "    \n",
    "    return r2_raw, true_norm, pred_norm\n",
    "\n",
    "\n",
    "# --------------------\n",
    "# 6. Main Training Function\n",
    "# --------------------\n",
    "def train_model(\n",
    "    train_dir, val_dir, teacher_paths, save_path,\n",
    "    gate_hidden=128, hint_lambda=5.0, weight_ratio=(0.6, 0.4),\n",
    "    hidden_dims=[128, 128], dropout=0.1,\n",
    "    epochs=1000, batch_size=64, lr=1e-3, min_lr=1e-4,\n",
    "    lr_patience=20, \n",
    "    seed=42,\n",
    "    no_save_epochs=200,\n",
    "    es_patience=100,\n",
    "    r2_effective_thresh=0.005,\n",
    "    best_r2_rel_thresh=0.005\n",
    "):\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU model: {torch.cuda.get_device_name(0)}\")\n",
    "    set_seed(seed)\n",
    "\n",
    "    # Load data\n",
    "    train_g = torch.load(os.path.join(train_dir, 'graph_data.pt'), map_location='cpu')\n",
    "    val_g = torch.load(os.path.join(val_dir, 'graph_data.pt'), map_location='cpu')\n",
    "    for i, g in enumerate(train_g):\n",
    "        g['idx'] = i\n",
    "    for i, g in enumerate(val_g):\n",
    "        g['idx'] = i\n",
    "\n",
    "    # Label normalization\n",
    "    ys = torch.stack([g['y'].float() for g in train_g]).view(-1)\n",
    "    y_mean, y_std = ys.mean().item(), ys.std().item() + 1e-8\n",
    "    for g in train_g + val_g:\n",
    "        g['y'] = (g['y'].float() - y_mean) / y_std\n",
    "        g.setdefault('y_soft', g['y'].clone())\n",
    "        g['y_soft'] = g['y_soft'].float()\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = create_data_loader(train_g, batch_size, shuffle=True, seed=seed)\n",
    "    val_loader = create_data_loader(val_g, batch_size, shuffle=False, seed=seed)\n",
    "    val_batch = next(iter(val_loader)) if len(val_loader) > 0 else None\n",
    "    tr_loader_pred = create_data_loader(train_g, batch_size, shuffle=False, seed=seed)\n",
    "    va_loader_pred = create_data_loader(val_g, batch_size, shuffle=False, seed=seed)\n",
    "\n",
    "    # Get input dimensions\n",
    "    sample = train_g[0]\n",
    "    n_dim = sample['x'].size(1) if isinstance(sample['x'], torch.Tensor) else len(sample['x'][0])\n",
    "    e_dim = sample.get('edge_attr', None).size(1) if (sample.get('edge_attr') is not None) else 0\n",
    "    g_dim = sample.get('u', None).size(1) if (sample.get('u') is not None) else 0\n",
    "\n",
    "    # Build student model\n",
    "    student = EnhancedGCN(n_dim, e_dim, g_dim, hidden_dims, dropout).to(device)\n",
    "    \n",
    "    # Load teacher models\n",
    "    teachers = []\n",
    "    for p in teacher_paths:\n",
    "        ck = torch.load(p, map_location=device)\n",
    "        t = SimpleGCN(\n",
    "            node_dim=ck['node_dim'], edge_dim=ck.get('edge_dim', 0),\n",
    "            global_dim=ck.get('global_dim', 0), hidden_dims=ck['hidden_dims']\n",
    "        ).to(device)\n",
    "        t.load_state_dict(ck['model_state_dict'], strict=True)\n",
    "        t.eval()\n",
    "        teachers.append(t)\n",
    "    print(f\"Loaded {len(teachers)} teacher models\")\n",
    "\n",
    "    # Optimizer settings\n",
    "    K = len(teachers)\n",
    "    gate = GateNet(student.final_dim, gate_hidden, K).to(device)\n",
    "    adapter = Adapter(student.final_dim, student.final_dim).to(device)\n",
    "    optimizer = optim.Adam(\n",
    "        params=list(student.parameters()) + list(gate.parameters()) + list(adapter.parameters()),\n",
    "        lr=lr, weight_decay=1e-5\n",
    "    )\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=lr_patience, min_lr=min_lr, verbose=True\n",
    "    )\n",
    "    scaler = GradScaler(enabled=(device.type == 'cuda'))\n",
    "\n",
    "    # Training history and early stopping\n",
    "    history = {'loss': [], 'train_r2': [], 'val_r2': []}\n",
    "    best_val_r2 = -float('inf')\n",
    "    best_train_r2 = -float('inf')\n",
    "    patience = 0\n",
    "    print(f\"\\nðŸš€ Start training (Total {epochs} epochs, seed={seed}) ===\")\n",
    "    print(f\"R2 effective improvement: absolute â‰¥{r2_effective_thresh} or relative â‰¥{best_r2_rel_thresh*100}% of best R2\")\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        student.train()\n",
    "        gate.train()\n",
    "        adapter.train()\n",
    "        total_loss = 0.0\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch:3d}/{epochs}\", leave=False)\n",
    "        for batch in pbar:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            with autocast(enabled=(device.type == 'cuda')):\n",
    "                pred_s, h_s = student(batch, return_feat=True)\n",
    "                with torch.no_grad():\n",
    "                    Ht = torch.stack([t(batch, return_feat=True)[1] for t in teachers], dim=1)\n",
    "                w = gate(h_s)\n",
    "                Ht_g = (w.unsqueeze(-1) * Ht).sum(dim=1)\n",
    "                loss_hint = F.mse_loss(adapter(h_s), Ht_g)\n",
    "                fused = (w * batch.y_soft).sum(dim=1)\n",
    "                loss_pred = F.mse_loss(pred_s, batch.y.view(-1))\n",
    "                loss_fused = F.mse_loss(pred_s, fused)\n",
    "                total_loss_batch = (\n",
    "                    weight_ratio[0] * loss_pred +\n",
    "                    weight_ratio[1] * loss_fused +\n",
    "                    hint_lambda * loss_hint\n",
    "                )\n",
    "            scaler.scale(total_loss_batch).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            total_loss += total_loss_batch.item() * batch.num_graphs\n",
    "            pbar.set_postfix({\"batch_loss\": f\"{total_loss_batch.item():.4f}\"})\n",
    "        pbar.close()\n",
    "\n",
    "        # Evaluation\n",
    "        avg_loss = total_loss / len(train_g)\n",
    "        train_r2, _, _ = eval_loader(train_loader, student, device)\n",
    "        val_r2, _, _ = eval_loader(val_loader, student, device)\n",
    "        history['loss'].append(avg_loss)\n",
    "        history['train_r2'].append(train_r2)\n",
    "        history['val_r2'].append(val_r2)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        # Effective improvement judgment\n",
    "        r2_effective = False\n",
    "        log_msg = \"\"\n",
    "        current_val_r2 = val_r2\n",
    "\n",
    "        # No save for first N epochs\n",
    "        if epoch <= no_save_epochs:\n",
    "            log_msg = f\"[First {no_save_epochs} epochs] Epoch {epoch:3d} | Train R2: {train_r2:.4f} | Val R2: {current_val_r2:.4f} (Not for optimal calculation)\"\n",
    "        \n",
    "        # Optimal model judgment after N epochs\n",
    "        else:\n",
    "            rel_r2_thresh = max(r2_effective_thresh, best_val_r2 * best_r2_rel_thresh)\n",
    "            \n",
    "            if current_val_r2 > best_val_r2:\n",
    "                r2_delta = current_val_r2 - best_val_r2\n",
    "                if r2_delta >= r2_effective_thresh or r2_delta >= rel_r2_thresh:\n",
    "                    r2_effective = True\n",
    "                    best_val_r2 = current_val_r2\n",
    "                    best_train_r2 = train_r2\n",
    "                    \n",
    "                    # Save best model\n",
    "                    student_hash = get_model_hash(student)\n",
    "                    gate_hash = get_model_hash(gate)\n",
    "                    adapter_hash = get_model_hash(adapter)\n",
    "                    with torch.no_grad():\n",
    "                        fixed_output = student(val_batch.to(device)).cpu().numpy() if val_batch is not None else None\n",
    "                    \n",
    "                    torch.save({\n",
    "                        'student_state_dict': student.state_dict(),\n",
    "                        'gate_state_dict': gate.state_dict(),\n",
    "                        'adapter_state_dict': adapter.state_dict(),\n",
    "                        'y_mean': y_mean, 'y_std': y_std,\n",
    "                        'node_dim': n_dim, 'edge_dim': e_dim, 'global_dim': g_dim,\n",
    "                        'hidden_dims': hidden_dims, 'dropout': dropout,\n",
    "                        'gate_hidden': gate_hidden, 'hint_lambda': hint_lambda,\n",
    "                        'weight_ratio': weight_ratio,\n",
    "                        'best_val_r2': best_val_r2,\n",
    "                        'best_train_r2': best_train_r2,\n",
    "                        'student_param_hash': student_hash,\n",
    "                        'gate_param_hash': gate_hash,\n",
    "                        'adapter_param_hash': adapter_hash,\n",
    "                        'fixed_output': fixed_output,\n",
    "                        'seed': seed,\n",
    "                        'saved_epoch': epoch,\n",
    "                        'batch_size': batch_size,\n",
    "                        'lr': lr\n",
    "                    }, save_path)\n",
    "                    log_msg = (f\"[Effective improvement] Epoch {epoch:3d} | \"\n",
    "                               f\"Train R2: {train_r2:.4f} | Val R2 updated to {best_val_r2:.4f} (Improvement {r2_delta:.4f}) | \"\n",
    "                               f\"Student model hash={student_hash[:8]}... | Model saved\")\n",
    "                else:\n",
    "                    log_msg = (f\"[Minor improvement] Epoch {epoch:3d} | \"\n",
    "                               f\"Train R2: {train_r2:.4f} | Val R2={current_val_r2:.4f} (Improvement {r2_delta:.4f} < Threshold) | \"\n",
    "                               f\"Current best Val R2={best_val_r2:.4f}\")\n",
    "            else:\n",
    "                log_msg = (f\"[No improvement] Epoch {epoch:3d} | \"\n",
    "                           f\"Train R2: {train_r2:.4f} | Val R2={current_val_r2:.4f} | \"\n",
    "                           f\"Current best Val R2={best_val_r2:.4f}\")\n",
    "\n",
    "        # Update early stopping counter\n",
    "        if epoch > no_save_epochs:\n",
    "            if r2_effective:\n",
    "                patience = 0\n",
    "                print(log_msg)\n",
    "            else:\n",
    "                patience += 1\n",
    "                patience_checks = [es_patience//4, es_patience//2, int(es_patience*0.75), es_patience]\n",
    "                if patience % 10 == 0 or patience in patience_checks:\n",
    "                    print(f\"[Early stopping counter] Epoch {epoch:3d} | No effective improvement for {patience}/{es_patience} epochs | \"\n",
    "                          f\"Train R2: {train_r2:.4f} | Current Val R2={current_val_r2:.4f} | Best Val R2={best_val_r2:.4f}\")\n",
    "        else:\n",
    "            if epoch % 10 == 0 or epoch == 1:\n",
    "                print(log_msg)\n",
    "\n",
    "        # Stage switch prompt\n",
    "        if epoch == no_save_epochs:\n",
    "            print(f\"\\n[Stage switch] Epoch {epoch:3d} | First {no_save_epochs} epochs ended, start optimal R2 calculation and model saving\")\n",
    "\n",
    "        # Trigger early stopping\n",
    "        if epoch > no_save_epochs and patience >= es_patience:\n",
    "            print(f\"\\nðŸ›‘ [Early stopping triggered] Epoch {epoch:3d} | No effective R2 improvement for {es_patience} consecutive epochs | \"\n",
    "                  f\"Best Val R2={best_val_r2:.4f} | Corresponding Train R2={best_train_r2:.4f}\")\n",
    "            break\n",
    "\n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(avg_loss)\n",
    "\n",
    "    # Plot training curves\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_r2'], label='Train R2', color='blue')\n",
    "    plt.plot(history['val_r2'], label='Val R2', color='orange')\n",
    "    plt.axhline(y=best_val_r2, color='red', linestyle='--', label=f'Best Val R2 ({best_val_r2:.4f})')\n",
    "    plt.axvline(x=no_save_epochs, color='gray', linestyle=':', label=f'Optimal calc start ({no_save_epochs})')\n",
    "    if os.path.exists(save_path):\n",
    "        best_epoch = torch.load(save_path)['saved_epoch']\n",
    "        plt.axvline(x=best_epoch, color='green', linestyle='-.', label=f'Best R2 epoch ({best_epoch})')\n",
    "        plt.scatter(best_epoch, best_val_r2, color='red', s=100, zorder=5)\n",
    "    plt.title('Training & Validation R2', fontname=\"Times New Roman\", fontsize=14)\n",
    "    plt.xlabel('Epoch', fontname=\"Times New Roman\", fontsize=12)\n",
    "    plt.ylabel('R2 Score', fontname=\"Times New Roman\", fontsize=12)\n",
    "    plt.legend(prop={\"family\": \"Times New Roman\"})\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['loss'], label='Training Loss', color='blue')\n",
    "    plt.axvline(x=no_save_epochs, color='gray', linestyle=':', label=f'Optimal calc start ({no_save_epochs})')\n",
    "    plt.title('Training Loss', fontname=\"Times New Roman\", fontsize=14)\n",
    "    plt.xlabel('Epoch', fontname=\"Times New Roman\", fontsize=12)\n",
    "    plt.ylabel('MSE Loss', fontname=\"Times New Roman\", fontsize=12)\n",
    "    plt.legend(prop={\"family\": \"Times New Roman\"})\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    curve_path = save_path.replace('.pt', '_curves.png')\n",
    "    plt.savefig(curve_path, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"Training curves saved to: {curve_path}\")\n",
    "\n",
    "    # Training completion prompt\n",
    "    if os.path.exists(save_path):\n",
    "        saved_info = torch.load(save_path)\n",
    "        print(f\"\\nðŸŽ¯ Training completed! Best Val R2: {best_val_r2:.4f} (Corresponding Train R2: {saved_info['best_train_r2']:.4f}), \"\n",
    "              f\"Model saved at {save_path} (Saved epoch: {saved_info['saved_epoch']})\")\n",
    "    else:\n",
    "        print(f\"\\nðŸŽ¯ Training completed! No model saved (Early stopped at epoch {epoch}, not exceeding {no_save_epochs} epochs or no effective improvement)\")\n",
    "\n",
    "    # Prediction and evaluation with best model\n",
    "    if os.path.exists(save_path):\n",
    "        save_dir = os.path.dirname(save_path)\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"Predict with best model (Epoch {saved_info['saved_epoch']})...\")\n",
    "        \n",
    "        train_r2, _, _ = predict_and_evaluate(\n",
    "            model=student, data_loader=tr_loader_pred,\n",
    "            y_mean=y_mean, y_std=y_std, device=device,\n",
    "            dataset_name=\"Train_Set\", save_dir=save_dir, seed=seed\n",
    "        )\n",
    "        \n",
    "        test_r2, _, _ = predict_and_evaluate(\n",
    "            model=student, data_loader=va_loader_pred,\n",
    "            y_mean=y_mean, y_std=y_std, device=device,\n",
    "            dataset_name=\"Test_Set\", save_dir=save_dir, seed=seed\n",
    "        )\n",
    "\n",
    "    return best_val_r2, save_path\n",
    "\n",
    "\n",
    "# --------------------\n",
    "# 7. Main Entry (Multi-seed Training)\n",
    "# --------------------\n",
    "if __name__ == '__main__':\n",
    "    # Hyperparameters\n",
    "    seeds = [0, 42, 100, 2025]\n",
    "    hint_lambdas = [1, 5, 10, 20]\n",
    "    weight_ratios = [(0.4, 0.6), (0.5,0.5), (0.6, 0.4), (0.7,0.3)]\n",
    "    gate_hiddens = [64, 128, 256]\n",
    "\n",
    "    # Path placeholders\n",
    "    train_dir = \"TRAIN_DATA_DIR_PATH\"\n",
    "    val_dir = \"VAL_DATA_DIR_PATH\"\n",
    "    teacher_paths = [\"TEACHER_MODEL_PATH_1\", \"TEACHER_MODEL_PATH_2\", \"TEACHER_MODEL_PATH_3\", \"TEACHER_MODEL_PATH_4\", \"TEACHER_MODEL_PATH_5\"]\n",
    "    save_root = \"MODEL_SAVE_ROOT_PATH\"\n",
    "    os.makedirs(save_root, exist_ok=True)\n",
    "\n",
    "    # Training parameters\n",
    "    epochs = 2000\n",
    "    batch_size = 32\n",
    "    lr = 1e-3\n",
    "    min_lr = 5e-5\n",
    "    lr_patience = 30\n",
    "    hidden_dims = [128, 128]\n",
    "    dropout = 0.1\n",
    "\n",
    "    # Run experiments\n",
    "    results = []\n",
    "    for hint_lambda in hint_lambdas:\n",
    "        for weight_ratio in weight_ratios:\n",
    "            for gate_hidden in gate_hiddens:\n",
    "                combo_key = f\"hl{hint_lambda}_wr{weight_ratio[0]}_{weight_ratio[1]}_gh{gate_hidden}\"\n",
    "                r2s = []\n",
    "                print(f\"\\n\" + \"=\"*50)\n",
    "                print(f\"Experiment combo: {combo_key}\")\n",
    "                print(\"=\"*50)\n",
    "                \n",
    "                for seed in seeds:\n",
    "                    print(f\"\\n-- Seed: {seed}\")\n",
    "                    set_seed(seed)\n",
    "                    save_path = os.path.join(save_root, f\"student_{combo_key}_seed{seed}.pt\")\n",
    "                    \n",
    "                    best_val_r2, _ = train_model(\n",
    "                        train_dir=train_dir,\n",
    "                        val_dir=val_dir,\n",
    "                        teacher_paths=teacher_paths,\n",
    "                        save_path=save_path,\n",
    "                        gate_hidden=gate_hidden,\n",
    "                        hint_lambda=hint_lambda,\n",
    "                        weight_ratio=weight_ratio,\n",
    "                        hidden_dims=hidden_dims,\n",
    "                        dropout=dropout,\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        lr=lr,\n",
    "                        min_lr=min_lr,\n",
    "                        lr_patience=lr_patience,\n",
    "                        seed=seed,\n",
    "                        no_save_epochs=200,\n",
    "                        es_patience=100,\n",
    "                        r2_effective_thresh=0.005,\n",
    "                        best_val_r2_rel_thresh=0.005\n",
    "                    )\n",
    "                    r2s.append(best_val_r2)\n",
    "                    \n",
    "                    # Clear GPU cache\n",
    "                    if torch.cuda.is_available():\n",
    "                        torch.cuda.empty_cache()\n",
    "                \n",
    "                # Summarize multi-seed results\n",
    "                mean_r2 = np.mean(r2s)\n",
    "                std_r2 = np.std(r2s) if len(r2s) > 1 else 0.0\n",
    "                print(f\"\\n\" + \"-\"*30)\n",
    "                print(f\"Combo {combo_key} results:\")\n",
    "                print(f\"  R2 list: {[f'{r:.4f}' for r in r2s]}\")\n",
    "                print(f\"  Mean R2: {mean_r2:.4f} | Std R2: {std_r2:.4f}\")\n",
    "                print(\"-\"*30)\n",
    "                \n",
    "                results.append({\n",
    "                    'combo': combo_key,\n",
    "                    'r2_mean': mean_r2,\n",
    "                    'r2_std': std_r2\n",
    "                })\n",
    "\n",
    "    # Save summary results\n",
    "    summary_path = os.path.join(save_root, 'results_summary.pkl')\n",
    "    with open(summary_path, 'wb') as f:\n",
    "        pickle.dump(results, f)\n",
    "    print(f\"\\nAll experiments completed! Summary saved to: {summary_path}\")\n",
    "\n",
    "    # Plot hyperparameter performance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    combos = [r['combo'] for r in results]\n",
    "    means = [r['r2_mean'] for r in results]\n",
    "    stds = [r['r2_std'] for r in results]\n",
    "    \n",
    "    sorted_idx = np.argsort(means)[::-1]\n",
    "    combos = [combos[i] for i in sorted_idx]\n",
    "    means = [means[i] for i in sorted_idx]\n",
    "    stds = [stds[i] for i in sorted_idx]\n",
    "    \n",
    "    bars = plt.bar(combos, means, yerr=stds, capsize=5, alpha=0.7, color='skyblue')\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=8)\n",
    "    plt.ylabel('Mean Validation R2', fontsize=12)\n",
    "    plt.title('Hyperparameter Combo Performance Ranking', fontsize=14)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    for bar, mean in zip(bars, means):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                 f'{mean:.4f}', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    hyper_param_path = os.path.join(save_root, 'hyper_param_performance.png')\n",
    "    plt.savefig(hyper_param_path, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"Hyperparameter performance plot saved to: {hyper_param_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn_gpu (GPU)",
   "language": "python",
   "name": "gnn_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
